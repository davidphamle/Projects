{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f596463a99a048efaba0f5bb1a3f7784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ccbaa0299afb4941bf55389df6727d4b",
              "IPY_MODEL_3f51061f6e0d4dc79104b458bc6d316a",
              "IPY_MODEL_547d9be2b29744e49510df65265077cd"
            ],
            "layout": "IPY_MODEL_fac2a60185a54b3b885e8a1c8436f843"
          }
        },
        "ccbaa0299afb4941bf55389df6727d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75e9ce90bf9b4e2183e3cd7ddc5b7626",
            "placeholder": "​",
            "style": "IPY_MODEL_7487d3a5a8a54cc3aa836cff63945903",
            "value": "Downloading builder script: "
          }
        },
        "3f51061f6e0d4dc79104b458bc6d316a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc847219079f4d80a3d9049aa4b48ea2",
            "max": 2169,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6439227202f4fe7ae876dfcbba0a399",
            "value": 2169
          }
        },
        "547d9be2b29744e49510df65265077cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_471475aa9885470788d489069c713baf",
            "placeholder": "​",
            "style": "IPY_MODEL_c10a6265b5db43ff9bc0a5702b76a24f",
            "value": " 5.65k/? [00:00&lt;00:00, 75.6kB/s]"
          }
        },
        "fac2a60185a54b3b885e8a1c8436f843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75e9ce90bf9b4e2183e3cd7ddc5b7626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7487d3a5a8a54cc3aa836cff63945903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc847219079f4d80a3d9049aa4b48ea2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6439227202f4fe7ae876dfcbba0a399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "471475aa9885470788d489069c713baf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c10a6265b5db43ff9bc0a5702b76a24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidphamle/Projects/blob/main/TeachingBot_Visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DIw9hEnIcCTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teaching Bot"
      ],
      "metadata": {
        "id": "hKJgHmluUoFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install & Import Libraries"
      ],
      "metadata": {
        "id": "k5_2XajQXt3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install PyPDF2\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken\n",
        "! pip install datasets transformers[sentencepiece]\n",
        "! pip install nltk rouge_score\n",
        "#!pip install pickle5\n",
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "wmaOi1QITSDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3RFl02m0EC5"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader, PdfWriter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "import sys\n",
        "import os\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_metric\n",
        "# import pickle5 as pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your API keys from openai, you will need to create an account.\n",
        "# Here is the link to get the keys: https://platform.openai.com/account/billing/overview\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"********************************************\""
      ],
      "metadata": {
        "id": "6el535ILbhuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/MyDrive/FYP\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmEVB7lztUWj",
        "outputId": "0400699d-e12a-4040-b089-bfe4586804b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# location of the pdf file/files.\n",
        "# reader = PdfReader('/content/gdrive/My Drive/Inputs/example1.pdf')\n",
        "\n",
        "merger = PdfWriter()\n",
        "\n",
        "\n",
        "output = open(\"document-output.pdf\", \"w+b\")\n",
        "\n",
        "for filename in os.listdir(root_dir):\n",
        "    if filename.endswith('.pdf'):\n",
        "        pdf_file = open(os.path.join(root_dir, filename), 'rb')\n",
        "        merger.append(pdf_file)\n",
        "\n",
        "\n",
        "merger.write(output)\n",
        "\n",
        "\n",
        "reader = PdfReader(output)\n",
        "\n",
        "# read data from the file and put them into a variable called raw_text\n",
        "raw_text = ''\n",
        "for i, page in enumerate(reader.pages):\n",
        "    text = page.extract_text()\n",
        "    if text:\n",
        "        raw_text += text"
      ],
      "metadata": {
        "id": "7EvbPU2YtmFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split loaded text into smaller chunks so that during information retrieval we don't hit the token size limits.\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    #separator = \"\\n\",\n",
        "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "LId1bIeky8tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFpTlF12X_N9",
        "outputId": "cc017cb9-bab2-4c5b-cee0-d0a0eac8c61b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "344"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "imDxas-uX_1Y",
        "outputId": "b120cee0-c897-42fc-d21d-92adfab222af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lecture 08: C, Input/Output  \\nFunctions  \\nAs a second -year subject, we assume that you are at least somewhat familiar with functions. Thus, \\nmost of this should be revision.  \\nArguments  \\nFunctions take arguments . It is likely at this stage that you have a basic understanding on how to use \\nfunctions including argument s. So why am I repeating myself. Well, this is a good opportunity to do \\nsome revision with a little bit more depth.  \\nSo lets begin with an example:  \\nint sum(int i1, int i2) \\n{ \\n int local = i1 + i2;  \\n return local; \\n} \\n \\nSum is a function (well duh) which takes two integers, applies some logic, and gives the caller back \\nanother integer. In this case, the name of the function and its return  value match wel l (sum the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "6ozd9EGjYDNx",
        "outputId": "6b754088-d7a4-49cf-b53b-df1f2bafccde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'another integer. In this case, the name of the function and its return  value match wel l (sum the \\nintegers given).  In terms of argument s, the above is an example of pass by copy. What does this mean?  \\nPass by Copy  \\nConsider the following ‘troll’ function and its corresponding ‘main’ call.  \\nint sum_troll(int i1, int i2) \\n{ \\n i1 = -7; \\n int local = i1 +  i2; \\n return local; \\n} \\n \\nint main() \\n{ \\n int a = 1; \\n int b = 2; \\n int c = sum_troll(a, b);  \\n // Print everything  \\n} \\n \\nAs we haven’t learned how to print in C, let’s just assume there is some code at the end of main  which \\nprints the values of ‘a’, ‘b’ and ‘c’. Now sum_troll  is an awful function. Instead of doing something \\nuseful, it returns somethings fairly useless (i2 – 7). In our call we might expect that ‘c’ should now'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "faiss similarity search"
      ],
      "metadata": {
        "id": "Ti1UpBye2U63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download embeddings from OpenAI\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "#create vector store\n",
        "db = FAISS.from_texts(texts, embeddings)\n",
        "\n",
        "\n",
        "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "L7ufKiBhzXYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#db.save_local(\"/content/gdrive/MyDrive/FAISS_TeachingBotIndex\",\"TeachingBotIndex\")\n",
        "# db.save_local(\"/content/gdrive/MyDrive/FAISS_TeachingBotIndex\",\"TeachingBotIndex\")\n",
        "\n",
        "# Save the vector store for later reuse\n",
        "# with open(\"/content/gdrive/MyDrive/FAISS_TeachingBotIndex/db.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(db, f)\n",
        "\n",
        "# Embedding Saved."
      ],
      "metadata": {
        "id": "Dw334kGX_NmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(db)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPQs-erVOjXm",
        "outputId": "f37c105e-7fb6-4810-cae2-681eb3f1a79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<langchain.vectorstores.faiss.FAISS object at 0x7c0f01836d10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Teach me about UNIX\"\n",
        "docs = db.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "KUvUkDLK1PIL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "7ac30cb7-b865-4b51-b22e-421f809713f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Unix is an operating system that is more friendly to programmers and advanced users. It has a design philosophy that \"everything is a file\" and it has a few common commands that can be used to accomplish tasks, such as \"ls\" to list the contents of the current directory, \"cp\" to copy a file, \"mv\" to move a file, \"rm\" to delete files, \"pwd\" to show the current working directory, \"cd\" to change the current directory, \"less\" to show a bit of a specified file, and \"cat\" to concatenate two files or show what is in one file. It is important to learn how to read the manual pages (\"man\") to understand how to use these commands. It also has a command \"chmod\" that can have catastrophic consequences if used incorrectly.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SeFjUDhAkafh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "wv6AGFYi1iT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c27f95b-e664-4d6b-82f8-a14974e4cff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='malloc  (it is faster).  \\nTo that end, it explains the existence of realloc . The signature is:  \\nvoid * realloc ( void * ptr, size_t size); \\n \\nIt takes a void*  which is a bit curious. The name is a bit of a give -away as to what it does. \\nUnsurprisingly, it stands for realloc ate. The pointer you give it will be the memory you wish to \\nreallocate. In essence, th e idea of realloc  is change the size of some memory you already have. If you \\ncan imagine that you have an array of size 10 and you decide (perhaps based on something you \\nlearned/will learn in ADSA) you now need the array to be size 20. Well, deallocating 1 0 integers and \\nthen reallocating the same 10 integers plus another 20 integers is a complete waste of time. Well,'),\n",
              " Document(page_content='up in nice, neat little packages (objects). The interactions of these packages can then be happily \\ndiscussed (cars drive on roads, users make withdrawals from bank accounts). C… is lower level than \\nthis. C programs are not usually that big (there are exceptions). Most people do not choose C for big \\nprograms. C is for small programs like hardware drivers or operating systems (admittedly not that \\nsmall) and other things which are  designed to be small and fast. Thus, while the C++ programmer \\nmight lean towards calloc  because it is inherently less buggy, the C programmer would mostly use \\nmalloc  (it is faster).  \\nTo that end, it explains the existence of realloc . The signature is:  \\nvoid * realloc ( void * ptr, size_t size);'),\n",
              " Document(page_content='To replace new  (or arguably what new replaced), C has three functions:  \\n• malloc  \\n• calloc  \\n• realloc  \\nInstead of delete , we have:  \\n• free So how do these work?  \\nWell… let’s start with malloc , which is the most common. It has  a function signature of:  \\nvoid * malloc ( size_t size); \\n \\nLucky, we have already thought about casting a bit. I mean… what is a void pointer  meant to be? \\nThe simplest explanation goes back to casting. All address es are addresses. So, the type of pointer is \\noften irrelevant (unless we dereference or use pointer arithmetic). In C, it was decided to create a \\nsingle function (well three) for allocating memory, leaving the managing of the types to the user. \\nThus, a very common thing to see is:  \\nint * a = (int *)(malloc(10 * sizeof( int)));'),\n",
              " Document(page_content='or 4 bytes (depending on operating system… another headache) we might reasonably expect that \\n‘my_ptr + 1’ will point at address 98 or 100 (depending on operating system). Likewise,  if the array \\nwere instead of the ‘unsigned long’ type (8 bytes) we might expect  ‘my_ptr + 1’ to point at address \\n104.  \\nMemory Allocation  \\nSimply put, C does not have new  and delete . Now, this does not mean that C lacks a Stack and Heap \\n(it has both). It simply means that we will need to use a new kind of syntax to achieve our dynamic \\nmemory requirements. It is quite ugly to use.  \\nTo replace new  (or arguably what new replaced), C has three functions:  \\n• malloc  \\n• calloc  \\n• realloc  \\nInstead of delete , we have:  \\n• free So how do these work?')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_divide(a, b):\n",
        "    return a / b if b != 0 else 0.0"
      ],
      "metadata": {
        "id": "hu7KRZm5ZbvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_metrics(queries, true_answers):\n",
        "    all_precision = []\n",
        "    all_recall = []\n",
        "    all_f1 = []\n",
        "\n",
        "    for query, true_answer in zip(queries, true_answers):\n",
        "        # Run similarity search on query\n",
        "        docs = db.similarity_search(query)\n",
        "\n",
        "        # Get model's answer\n",
        "        model_answer = chain.run(input_documents=docs, question=query)\n",
        "        # print(model_answer)\n",
        "        # print(true_answer)\n",
        "\n",
        "        # Tokenize true and model answers (splitting by comma and trimming spaces)\n",
        "        true_tokens = set(true_answer.split(\", \"))\n",
        "        model_tokens = set(model_answer.split(\", \"))\n",
        "\n",
        "        # Convert sets to binary label format for sklearn metrics\n",
        "        all_tokens = list(set(true_tokens) | set(model_tokens))\n",
        "        t_labels = [1 if token in true_tokens else 0 for token in all_tokens]\n",
        "        m_labels = [1 if token in model_tokens else 0 for token in all_tokens]\n",
        "        # print(all_tokens)\n",
        "        # print(t_labels)\n",
        "        # print(m_labels)\n",
        "\n",
        "        # Compute metrics\n",
        "        precision = precision_score(t_labels, m_labels)\n",
        "        recall = recall_score(t_labels, m_labels)\n",
        "        f1 = f1_score(t_labels, m_labels)\n",
        "\n",
        "        all_precision.append(precision)\n",
        "        all_recall.append(recall)\n",
        "        all_f1.append(f1)\n",
        "\n",
        "        # confusion_matrix = metrics.confusion_matrix(t_labels, m_labels)\n",
        "        # cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
        "        # cm_display.plot()\n",
        "        # plt.show()\n",
        "\n",
        "        # true_positive = confusion_matrix[1,1]\n",
        "        # true_negative = confusion_matrix[0,0]\n",
        "        # false_positive = confusion_matrix[0,1]\n",
        "        # false_negative = confusion_matrix[1,0]\n",
        "\n",
        "        # positive_Precision = safe_divide(true_positive, (true_positive + false_positive))\n",
        "        # positive_Recall = safe_divide(true_positive, (true_positive + false_negative))\n",
        "\n",
        "        # positive_F1 = safe_divide(2 * (positive_Precision * positive_Recall), (positive_Precision + positive_Recall))\n",
        "\n",
        "        # negative_Precision = safe_divide(true_negative, (true_negative + false_negative))\n",
        "        # negative_Recall = safe_divide(true_negative, (true_negative + false_positive))\n",
        "\n",
        "        # negative_F1 = safe_divide(2 * (negative_Precision * negative_Recall), (negative_Precision + negative_Recall))\n",
        "\n",
        "        # print(f\"Positive_F1: {positive_F1}\")\n",
        "        # print(f\"Negative_F1: {negative_F1}\")\n",
        "\n",
        "    return all_precision, all_recall, all_f1\n",
        "\n",
        "# Queries and true answers\n",
        "queries = [\n",
        "    \"What is the pipe operator?\",\n",
        "    \"What does makefile do?\",\n",
        "    \"What is the structure of a minimalist makefile?\",\n",
        "    \"What is a race condition?\",\n",
        "    \"What is a signal handler?\",\n",
        "    \"What is pthread_cond_wait?\",\n",
        "    \"What happens when you establish a connection to a computer via a network?\"\n",
        "    \"Teach me about UNIX.\",\n",
        "    \"What are Processes?\",\n",
        "    \"Explain INodes to me\",\n",
        "    \"What's the difference between Asynchronous and Synchronous IO in the lectures given?\",\n",
        "    \"What are sockets used for?\",\n",
        "    \"Explain pipes like I’m 5.\",\n",
        "    \"How to implement pipes in C?\",\n",
        "    \"What is the difference between a thread and a process?\",\n",
        "    \"How does one glob?\",\n",
        "    \"How do you use realloc in C?\"\n",
        "]\n",
        "true_answers = [\n",
        "    \"Pipes serve as a form of redirection that exists between programs. Instead of reading from a file (which is essentially a finite process once the file runs out) or writing to a file, pipes allow one process to dynamically read the output of a second process and use it as input.\",\n",
        "    \"Make is all about date-stamps. A date-stamp tells you when a file was last modified and (usually) an unmodified file does not need to be recompiled. Actually, this isn’t strictly true. There are two reasons to recompile a file. Number one is that it changed (hence date stamps). Number two is that something it relies upon (a dependency) changed. So, the game of Make is to keep track of what files have changed (which is an easy operation using the operating system) and which files depend upon which files… a less easy operation.\",\n",
        "    \"A Minimalist Makefile Looks like this: my_exe: my_code.c my_code.h gcc my_code.c -o my_exe This reads: • Make a file called my_exe from my_code.c (the gcc command) • Do this if someone types: make my_exe • Only do something if my_exe is older than my_code.c or my_code.h. Otherwise… do nothing.\",\n",
        "    \"Simply put, a race condition is when two independent ‘tasks’ which are interdependent produce different results depending on the timing of how those two tasks are implemented.\",\n",
        "    \"Signal handling in C is very straight forward and consists of two main parts (actually, quite similar to threads). There is the ‘setup’ part where you specify when in the code you want the signal handler to become active. Then there is the function which is called by the signal.\",\n",
        "    \"In C, the way to implement this ‘signal-based’ method of notifying threads is: pthread_cond_wait(&cond, &lock) The idea here is that the thread which is to read from shared memory needs to wait for the data to be populated. That data is associated with a mutex (&lock) and a ‘pthread_cond_t’ (&cond) which is a kind of condition. In order to call pthread_cond_wait the waiting thread (consumer) must first have the mutex. But wait? If the consumer locks the mutex, how will the producer deposit the data into shared memory? Doesn’t it need the mutex first. Fortunately (i.e. by design), calling pthread_cond_wait unlocks the mutex. Now the producer is free to do its thing, lock the mutex, change some data.\",\n",
        "    \"The traditional way of connecting to another computer across a network is via an IP address. Briefly, an IP address is an address similar to a house address. It tells you where the information is meant to go to/come from. Along with the IP address is the port number. The port number is used to determine what is being communicated, a bit like a radio frequency. So… IP Address: • Unique for a computer • Several processes can communicate using the same IP address (i.e. same computer) Port Number: • Unique for an application • Several computers can communicate using the same Port number (i.e. same application).\",\n",
        "    \"Unix is a multiuser, multitasking operating system (OS) designed for flexibility and adaptability\",\n",
        "    \"The term process (Job) refers to program code that has been loaded into a computer's memory so that it can be executed by the central processing unit (CPU).\",\n",
        "    \"By definition, an inode is an index node. It serves as a unique identifier for a specific piece of metadata on a given filesystem. Each piece of metadata describes what we think of as a file.\",\n",
        "    \"Synchronous input/output (I/O) occurs while you wait. Applications processing cannot continue until the I/O operation is complete. In contrast, asynchronous I/O (AIO) operations run in the background and do not block user applications.\",\n",
        "    \"Mainly for client and server interaction.\",\n",
        "    \"A pipe simply refers to a temporary software connection between two programs or commands. An area of the main memory is treated like a virtual file to temporarily hold data and pass it from one process to another in a single direction.\",\n",
        "    \"To create a simple pipe with C, we make use of the pipe() system call. It takes a single argument, which is an array of two integers, and if successful, the array will contain two new file descriptors to be used for the pipeline.\",\n",
        "    \"Process is the program under action whereas a thread is the smallest segment of instructions that can be handled independently by a scheduler.\",\n",
        "    \"In computer programming, glob (/ɡlɒb/) patterns specify sets of filenames with wildcard characters. For example, the Unix Bash shell command mv *. txt textfiles/ moves ( mv ) all files with names ending in . txt from the current directory to the directory textfiles.\",\n",
        "    \"ptr = realloc (ptr,newsize); The above statement allocates a new memory space with a specified size in the variable newsize. After executing the function, the pointer will be returned to the first byte of the memory block. The new size can be larger or smaller than the previous memory.\"\n",
        "]\n",
        "\n",
        "# Run the evaluation\n",
        "all_precision, all_recall, all_f1 = evaluate_metrics(queries, true_answers)\n",
        "print(f'Precision: {all_precision}, Recall: {all_recall}, F1-measure: {all_f1}')\n"
      ],
      "metadata": {
        "id": "VeiESAUvojdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3fafa0f-b409-41a0-f1a1-204c405717da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Recall: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], F1-measure: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "def evaluate_ROGUE_metrics(queries, true_answers):\n",
        "    all_rouge = []\n",
        "\n",
        "    for query, true_answer in zip(queries, true_answers):\n",
        "        # Simulated model's answer (replace this with your actual model)\n",
        "        docs = db.similarity_search(query)\n",
        "        model_answer = docs[0]  # Simulated answer, replace with actual model output\n",
        "\n",
        "        # Compute ROUGE scores\n",
        "        rouge_scores = rouge.compute(predictions=[model_answer], references=[true_answer])\n",
        "        all_rouge.append(rouge_scores)\n",
        "\n",
        "        # The rest of your code (confusion matrix etc.) remains the same\n",
        "\n",
        "    return all_rouge\n",
        "\n",
        "# Replace queries and true_answers with your actual data\n",
        "queries = [\n",
        "    \"What is the pipe operator?\",\n",
        "    \"What does makefile do?\",\n",
        "    \"What is the structure of a minimalist makefile?\",\n",
        "    \"What is a race condition?\",\n",
        "    \"What is a signal handler?\",\n",
        "    \"What is pthread_cond_wait?\",\n",
        "    \"What happens when you establish a connection to a computer via a network?\"\n",
        "    \"Teach me about UNIX.\",\n",
        "    \"What are Processes?\",\n",
        "    \"Explain INodes to me\",\n",
        "    \"What's the difference between Asynchronous and Synchronous IO in the lectures given?\",\n",
        "    \"What are sockets used for?\",\n",
        "    \"Explain pipes like I’m 5.\",\n",
        "    \"How to implement pipes in C?\",\n",
        "    \"What is the difference between a thread and a process?\",\n",
        "    \"How does one glob?\",\n",
        "    \"How do you use realloc in C?\"\n",
        "]\n",
        "true_answers = [\n",
        "    \"Pipes serve as a form of redirection that exists between programs. Instead of reading from a file (which is essentially a finite process once the file runs out) or writing to a file, pipes allow one process to dynamically read the output of a second process and use it as input.\",\n",
        "    \"Make is all about date-stamps. A date-stamp tells you when a file was last modified and (usually) an unmodified file does not need to be recompiled. Actually, this isn’t strictly true. There are two reasons to recompile a file. Number one is that it changed (hence date stamps). Number two is that something it relies upon (a dependency) changed. So, the game of Make is to keep track of what files have changed (which is an easy operation using the operating system) and which files depend upon which files… a less easy operation.\",\n",
        "    \"A Minimalist Makefile Looks like this: my_exe: my_code.c my_code.h gcc my_code.c -o my_exe This reads: • Make a file called my_exe from my_code.c (the gcc command) • Do this if someone types: make my_exe • Only do something if my_exe is older than my_code.c or my_code.h. Otherwise… do nothing.\",\n",
        "    \"Simply put, a race condition is when two independent ‘tasks’ which are interdependent produce different results depending on the timing of how those two tasks are implemented.\",\n",
        "    \"Signal handling in C is very straight forward and consists of two main parts (actually, quite similar to threads). There is the ‘setup’ part where you specify when in the code you want the signal handler to become active. Then there is the function which is called by the signal.\",\n",
        "    \"In C, the way to implement this ‘signal-based’ method of notifying threads is: pthread_cond_wait(&cond, &lock) The idea here is that the thread which is to read from shared memory needs to wait for the data to be populated. That data is associated with a mutex (&lock) and a ‘pthread_cond_t’ (&cond) which is a kind of condition. In order to call pthread_cond_wait the waiting thread (consumer) must first have the mutex. But wait? If the consumer locks the mutex, how will the producer deposit the data into shared memory? Doesn’t it need the mutex first. Fortunately (i.e. by design), calling pthread_cond_wait unlocks the mutex. Now the producer is free to do its thing, lock the mutex, change some data.\",\n",
        "    \"The traditional way of connecting to another computer across a network is via an IP address. Briefly, an IP address is an address similar to a house address. It tells you where the information is meant to go to/come from. Along with the IP address is the port number. The port number is used to determine what is being communicated, a bit like a radio frequency. So… IP Address: • Unique for a computer • Several processes can communicate using the same IP address (i.e. same computer) Port Number: • Unique for an application • Several computers can communicate using the same Port number (i.e. same application).\",\n",
        "    \"Unix is a multiuser, multitasking operating system (OS) designed for flexibility and adaptability\",\n",
        "    \"The term process (Job) refers to program code that has been loaded into a computer's memory so that it can be executed by the central processing unit (CPU).\",\n",
        "    \"By definition, an inode is an index node. It serves as a unique identifier for a specific piece of metadata on a given filesystem. Each piece of metadata describes what we think of as a file.\",\n",
        "    \"Synchronous input/output (I/O) occurs while you wait. Applications processing cannot continue until the I/O operation is complete. In contrast, asynchronous I/O (AIO) operations run in the background and do not block user applications.\",\n",
        "    \"Mainly for client and server interaction.\",\n",
        "    \"A pipe simply refers to a temporary software connection between two programs or commands. An area of the main memory is treated like a virtual file to temporarily hold data and pass it from one process to another in a single direction.\",\n",
        "    \"To create a simple pipe with C, we make use of the pipe() system call. It takes a single argument, which is an array of two integers, and if successful, the array will contain two new file descriptors to be used for the pipeline.\",\n",
        "    \"Process is the program under action whereas a thread is the smallest segment of instructions that can be handled independently by a scheduler.\",\n",
        "    \"In computer programming, glob (/ɡlɒb/) patterns specify sets of filenames with wildcard characters. For example, the Unix Bash shell command mv *. txt textfiles/ moves ( mv ) all files with names ending in . txt from the current directory to the directory textfiles.\",\n",
        "    \"ptr = realloc (ptr,newsize); The above statement allocates a new memory space with a specified size in the variable newsize. After executing the function, the pointer will be returned to the first byte of the memory block. The new size can be larger or smaller than the previous memory.\"\n",
        "]\n",
        "\n",
        "# Run the evaluation\n",
        "all_rouge = evaluate_ROGUE_metrics(queries, true_answers)\n",
        "print(f'ROUGE Scores: {all_rouge}')"
      ],
      "metadata": {
        "id": "gFwqkbGGZM0R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "f596463a99a048efaba0f5bb1a3f7784",
            "ccbaa0299afb4941bf55389df6727d4b",
            "3f51061f6e0d4dc79104b458bc6d316a",
            "547d9be2b29744e49510df65265077cd",
            "fac2a60185a54b3b885e8a1c8436f843",
            "75e9ce90bf9b4e2183e3cd7ddc5b7626",
            "7487d3a5a8a54cc3aa836cff63945903",
            "cc847219079f4d80a3d9049aa4b48ea2",
            "f6439227202f4fe7ae876dfcbba0a399",
            "471475aa9885470788d489069c713baf",
            "c10a6265b5db43ff9bc0a5702b76a24f"
          ]
        },
        "outputId": "67db5cef-19a5-4124-cfe4-284d7d216795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-35708b4c84e8>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge = load_metric(\"rouge\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f596463a99a048efaba0f5bb1a3f7784"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores: [{'rouge1': AggregateScore(low=Score(precision=0.28688524590163933, recall=0.6862745098039216, fmeasure=0.4046242774566474), mid=Score(precision=0.28688524590163933, recall=0.6862745098039216, fmeasure=0.4046242774566474), high=Score(precision=0.28688524590163933, recall=0.6862745098039216, fmeasure=0.4046242774566474)), 'rouge2': AggregateScore(low=Score(precision=0.15702479338842976, recall=0.38, fmeasure=0.2222222222222222), mid=Score(precision=0.15702479338842976, recall=0.38, fmeasure=0.2222222222222222), high=Score(precision=0.15702479338842976, recall=0.38, fmeasure=0.2222222222222222)), 'rougeL': AggregateScore(low=Score(precision=0.14754098360655737, recall=0.35294117647058826, fmeasure=0.20809248554913296), mid=Score(precision=0.14754098360655737, recall=0.35294117647058826, fmeasure=0.20809248554913296), high=Score(precision=0.14754098360655737, recall=0.35294117647058826, fmeasure=0.20809248554913296)), 'rougeLsum': AggregateScore(low=Score(precision=0.14754098360655737, recall=0.35294117647058826, fmeasure=0.20809248554913296), mid=Score(precision=0.14754098360655737, recall=0.35294117647058826, fmeasure=0.20809248554913296), high=Score(precision=0.14754098360655737, recall=0.35294117647058826, fmeasure=0.20809248554913296))}, {'rouge1': AggregateScore(low=Score(precision=0.2578125, recall=0.34375, fmeasure=0.29464285714285715), mid=Score(precision=0.2578125, recall=0.34375, fmeasure=0.29464285714285715), high=Score(precision=0.2578125, recall=0.34375, fmeasure=0.29464285714285715)), 'rouge2': AggregateScore(low=Score(precision=0.03937007874015748, recall=0.05263157894736842, fmeasure=0.04504504504504505), mid=Score(precision=0.03937007874015748, recall=0.05263157894736842, fmeasure=0.04504504504504505), high=Score(precision=0.03937007874015748, recall=0.05263157894736842, fmeasure=0.04504504504504505)), 'rougeL': AggregateScore(low=Score(precision=0.1171875, recall=0.15625, fmeasure=0.13392857142857142), mid=Score(precision=0.1171875, recall=0.15625, fmeasure=0.13392857142857142), high=Score(precision=0.1171875, recall=0.15625, fmeasure=0.13392857142857142)), 'rougeLsum': AggregateScore(low=Score(precision=0.1171875, recall=0.15625, fmeasure=0.13392857142857142), mid=Score(precision=0.1171875, recall=0.15625, fmeasure=0.13392857142857142), high=Score(precision=0.1171875, recall=0.15625, fmeasure=0.13392857142857142))}, {'rouge1': AggregateScore(low=Score(precision=0.379746835443038, recall=0.9523809523809523, fmeasure=0.5429864253393665), mid=Score(precision=0.379746835443038, recall=0.9523809523809523, fmeasure=0.5429864253393665), high=Score(precision=0.379746835443038, recall=0.9523809523809523, fmeasure=0.5429864253393665)), 'rouge2': AggregateScore(low=Score(precision=0.3248407643312102, recall=0.8225806451612904, fmeasure=0.4657534246575342), mid=Score(precision=0.3248407643312102, recall=0.8225806451612904, fmeasure=0.4657534246575342), high=Score(precision=0.3248407643312102, recall=0.8225806451612904, fmeasure=0.4657534246575342)), 'rougeL': AggregateScore(low=Score(precision=0.3670886075949367, recall=0.9206349206349206, fmeasure=0.5248868778280542), mid=Score(precision=0.3670886075949367, recall=0.9206349206349206, fmeasure=0.5248868778280542), high=Score(precision=0.3670886075949367, recall=0.9206349206349206, fmeasure=0.5248868778280542)), 'rougeLsum': AggregateScore(low=Score(precision=0.3670886075949367, recall=0.9206349206349206, fmeasure=0.5248868778280542), mid=Score(precision=0.3670886075949367, recall=0.9206349206349206, fmeasure=0.5248868778280542), high=Score(precision=0.3670886075949367, recall=0.9206349206349206, fmeasure=0.5248868778280542))}, {'rouge1': AggregateScore(low=Score(precision=0.18796992481203006, recall=0.9259259259259259, fmeasure=0.3125), mid=Score(precision=0.18796992481203006, recall=0.9259259259259259, fmeasure=0.3125), high=Score(precision=0.18796992481203006, recall=0.9259259259259259, fmeasure=0.3125)), 'rouge2': AggregateScore(low=Score(precision=0.17424242424242425, recall=0.8846153846153846, fmeasure=0.2911392405063291), mid=Score(precision=0.17424242424242425, recall=0.8846153846153846, fmeasure=0.2911392405063291), high=Score(precision=0.17424242424242425, recall=0.8846153846153846, fmeasure=0.2911392405063291)), 'rougeL': AggregateScore(low=Score(precision=0.18796992481203006, recall=0.9259259259259259, fmeasure=0.3125), mid=Score(precision=0.18796992481203006, recall=0.9259259259259259, fmeasure=0.3125), high=Score(precision=0.18796992481203006, recall=0.9259259259259259, fmeasure=0.3125)), 'rougeLsum': AggregateScore(low=Score(precision=0.18796992481203006, recall=0.9259259259259259, fmeasure=0.3125), mid=Score(precision=0.18796992481203006, recall=0.9259259259259259, fmeasure=0.3125), high=Score(precision=0.18796992481203006, recall=0.9259259259259259, fmeasure=0.3125))}, {'rouge1': AggregateScore(low=Score(precision=0.10559006211180125, recall=0.34, fmeasure=0.16113744075829384), mid=Score(precision=0.10559006211180125, recall=0.34, fmeasure=0.16113744075829384), high=Score(precision=0.10559006211180125, recall=0.34, fmeasure=0.16113744075829384)), 'rouge2': AggregateScore(low=Score(precision=0.00625, recall=0.02040816326530612, fmeasure=0.00956937799043062), mid=Score(precision=0.00625, recall=0.02040816326530612, fmeasure=0.00956937799043062), high=Score(precision=0.00625, recall=0.02040816326530612, fmeasure=0.00956937799043062)), 'rougeL': AggregateScore(low=Score(precision=0.062111801242236024, recall=0.2, fmeasure=0.09478672985781991), mid=Score(precision=0.062111801242236024, recall=0.2, fmeasure=0.09478672985781991), high=Score(precision=0.062111801242236024, recall=0.2, fmeasure=0.09478672985781991)), 'rougeLsum': AggregateScore(low=Score(precision=0.062111801242236024, recall=0.2, fmeasure=0.09478672985781991), mid=Score(precision=0.062111801242236024, recall=0.2, fmeasure=0.09478672985781991), high=Score(precision=0.062111801242236024, recall=0.2, fmeasure=0.09478672985781991))}, {'rouge1': AggregateScore(low=Score(precision=0.773972602739726, recall=0.8692307692307693, fmeasure=0.8188405797101449), mid=Score(precision=0.773972602739726, recall=0.8692307692307693, fmeasure=0.8188405797101449), high=Score(precision=0.773972602739726, recall=0.8692307692307693, fmeasure=0.8188405797101449)), 'rouge2': AggregateScore(low=Score(precision=0.6827586206896552, recall=0.7674418604651163, fmeasure=0.7226277372262774), mid=Score(precision=0.6827586206896552, recall=0.7674418604651163, fmeasure=0.7226277372262774), high=Score(precision=0.6827586206896552, recall=0.7674418604651163, fmeasure=0.7226277372262774)), 'rougeL': AggregateScore(low=Score(precision=0.7397260273972602, recall=0.8307692307692308, fmeasure=0.7826086956521738), mid=Score(precision=0.7397260273972602, recall=0.8307692307692308, fmeasure=0.7826086956521738), high=Score(precision=0.7397260273972602, recall=0.8307692307692308, fmeasure=0.7826086956521738)), 'rougeLsum': AggregateScore(low=Score(precision=0.7397260273972602, recall=0.8307692307692308, fmeasure=0.7826086956521738), mid=Score(precision=0.7397260273972602, recall=0.8307692307692308, fmeasure=0.7826086956521738), high=Score(precision=0.7397260273972602, recall=0.8307692307692308, fmeasure=0.7826086956521738))}, {'rouge1': AggregateScore(low=Score(precision=0.33587786259541985, recall=0.41509433962264153, fmeasure=0.3713080168776371), mid=Score(precision=0.33587786259541985, recall=0.41509433962264153, fmeasure=0.3713080168776371), high=Score(precision=0.33587786259541985, recall=0.41509433962264153, fmeasure=0.3713080168776371)), 'rouge2': AggregateScore(low=Score(precision=0.2, recall=0.24761904761904763, fmeasure=0.22127659574468084), mid=Score(precision=0.2, recall=0.24761904761904763, fmeasure=0.22127659574468084), high=Score(precision=0.2, recall=0.24761904761904763, fmeasure=0.22127659574468084)), 'rougeL': AggregateScore(low=Score(precision=0.20610687022900764, recall=0.25471698113207547, fmeasure=0.2278481012658228), mid=Score(precision=0.20610687022900764, recall=0.25471698113207547, fmeasure=0.2278481012658228), high=Score(precision=0.20610687022900764, recall=0.25471698113207547, fmeasure=0.2278481012658228)), 'rougeLsum': AggregateScore(low=Score(precision=0.20610687022900764, recall=0.25471698113207547, fmeasure=0.2278481012658228), mid=Score(precision=0.20610687022900764, recall=0.25471698113207547, fmeasure=0.2278481012658228), high=Score(precision=0.20610687022900764, recall=0.25471698113207547, fmeasure=0.2278481012658228))}, {'rouge1': AggregateScore(low=Score(precision=0.027777777777777776, recall=0.3076923076923077, fmeasure=0.050955414012738856), mid=Score(precision=0.027777777777777776, recall=0.3076923076923077, fmeasure=0.050955414012738856), high=Score(precision=0.027777777777777776, recall=0.3076923076923077, fmeasure=0.050955414012738856)), 'rouge2': AggregateScore(low=Score(precision=0.006993006993006993, recall=0.08333333333333333, fmeasure=0.012903225806451613), mid=Score(precision=0.006993006993006993, recall=0.08333333333333333, fmeasure=0.012903225806451613), high=Score(precision=0.006993006993006993, recall=0.08333333333333333, fmeasure=0.012903225806451613)), 'rougeL': AggregateScore(low=Score(precision=0.027777777777777776, recall=0.3076923076923077, fmeasure=0.050955414012738856), mid=Score(precision=0.027777777777777776, recall=0.3076923076923077, fmeasure=0.050955414012738856), high=Score(precision=0.027777777777777776, recall=0.3076923076923077, fmeasure=0.050955414012738856)), 'rougeLsum': AggregateScore(low=Score(precision=0.027777777777777776, recall=0.3076923076923077, fmeasure=0.050955414012738856), mid=Score(precision=0.027777777777777776, recall=0.3076923076923077, fmeasure=0.050955414012738856), high=Score(precision=0.027777777777777776, recall=0.3076923076923077, fmeasure=0.050955414012738856))}, {'rouge1': AggregateScore(low=Score(precision=0.07042253521126761, recall=0.3448275862068966, fmeasure=0.11695906432748539), mid=Score(precision=0.07042253521126761, recall=0.3448275862068966, fmeasure=0.11695906432748539), high=Score(precision=0.07042253521126761, recall=0.3448275862068966, fmeasure=0.11695906432748539)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.04929577464788732, recall=0.2413793103448276, fmeasure=0.08187134502923975), mid=Score(precision=0.04929577464788732, recall=0.2413793103448276, fmeasure=0.08187134502923975), high=Score(precision=0.04929577464788732, recall=0.2413793103448276, fmeasure=0.08187134502923975)), 'rougeLsum': AggregateScore(low=Score(precision=0.04929577464788732, recall=0.2413793103448276, fmeasure=0.08187134502923975), mid=Score(precision=0.04929577464788732, recall=0.2413793103448276, fmeasure=0.08187134502923975), high=Score(precision=0.04929577464788732, recall=0.2413793103448276, fmeasure=0.08187134502923975))}, {'rouge1': AggregateScore(low=Score(precision=0.064, recall=0.2222222222222222, fmeasure=0.09937888198757763), mid=Score(precision=0.064, recall=0.2222222222222222, fmeasure=0.09937888198757763), high=Score(precision=0.064, recall=0.2222222222222222, fmeasure=0.09937888198757763)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.04, recall=0.1388888888888889, fmeasure=0.062111801242236024), mid=Score(precision=0.04, recall=0.1388888888888889, fmeasure=0.062111801242236024), high=Score(precision=0.04, recall=0.1388888888888889, fmeasure=0.062111801242236024)), 'rougeLsum': AggregateScore(low=Score(precision=0.04, recall=0.1388888888888889, fmeasure=0.062111801242236024), mid=Score(precision=0.04, recall=0.1388888888888889, fmeasure=0.062111801242236024), high=Score(precision=0.04, recall=0.1388888888888889, fmeasure=0.062111801242236024))}, {'rouge1': AggregateScore(low=Score(precision=0.05925925925925926, recall=0.21621621621621623, fmeasure=0.0930232558139535), mid=Score(precision=0.05925925925925926, recall=0.21621621621621623, fmeasure=0.0930232558139535), high=Score(precision=0.05925925925925926, recall=0.21621621621621623, fmeasure=0.0930232558139535)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.037037037037037035, recall=0.13513513513513514, fmeasure=0.05813953488372093), mid=Score(precision=0.037037037037037035, recall=0.13513513513513514, fmeasure=0.05813953488372093), high=Score(precision=0.037037037037037035, recall=0.13513513513513514, fmeasure=0.05813953488372093)), 'rougeLsum': AggregateScore(low=Score(precision=0.037037037037037035, recall=0.13513513513513514, fmeasure=0.05813953488372093), mid=Score(precision=0.037037037037037035, recall=0.13513513513513514, fmeasure=0.05813953488372093), high=Score(precision=0.037037037037037035, recall=0.13513513513513514, fmeasure=0.05813953488372093))}, {'rouge1': AggregateScore(low=Score(precision=0.01639344262295082, recall=0.3333333333333333, fmeasure=0.03125), mid=Score(precision=0.01639344262295082, recall=0.3333333333333333, fmeasure=0.03125), high=Score(precision=0.01639344262295082, recall=0.3333333333333333, fmeasure=0.03125)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.01639344262295082, recall=0.3333333333333333, fmeasure=0.03125), mid=Score(precision=0.01639344262295082, recall=0.3333333333333333, fmeasure=0.03125), high=Score(precision=0.01639344262295082, recall=0.3333333333333333, fmeasure=0.03125)), 'rougeLsum': AggregateScore(low=Score(precision=0.01639344262295082, recall=0.3333333333333333, fmeasure=0.03125), mid=Score(precision=0.01639344262295082, recall=0.3333333333333333, fmeasure=0.03125), high=Score(precision=0.01639344262295082, recall=0.3333333333333333, fmeasure=0.03125))}, {'rouge1': AggregateScore(low=Score(precision=0.16541353383458646, recall=0.5238095238095238, fmeasure=0.2514285714285714), mid=Score(precision=0.16541353383458646, recall=0.5238095238095238, fmeasure=0.2514285714285714), high=Score(precision=0.16541353383458646, recall=0.5238095238095238, fmeasure=0.2514285714285714)), 'rouge2': AggregateScore(low=Score(precision=0.03787878787878788, recall=0.12195121951219512, fmeasure=0.05780346820809249), mid=Score(precision=0.03787878787878788, recall=0.12195121951219512, fmeasure=0.05780346820809249), high=Score(precision=0.03787878787878788, recall=0.12195121951219512, fmeasure=0.05780346820809249)), 'rougeL': AggregateScore(low=Score(precision=0.09022556390977443, recall=0.2857142857142857, fmeasure=0.13714285714285715), mid=Score(precision=0.09022556390977443, recall=0.2857142857142857, fmeasure=0.13714285714285715), high=Score(precision=0.09022556390977443, recall=0.2857142857142857, fmeasure=0.13714285714285715)), 'rougeLsum': AggregateScore(low=Score(precision=0.09022556390977443, recall=0.2857142857142857, fmeasure=0.13714285714285715), mid=Score(precision=0.09022556390977443, recall=0.2857142857142857, fmeasure=0.13714285714285715), high=Score(precision=0.09022556390977443, recall=0.2857142857142857, fmeasure=0.13714285714285715))}, {'rouge1': AggregateScore(low=Score(precision=0.1323529411764706, recall=0.4090909090909091, fmeasure=0.19999999999999998), mid=Score(precision=0.1323529411764706, recall=0.4090909090909091, fmeasure=0.19999999999999998), high=Score(precision=0.1323529411764706, recall=0.4090909090909091, fmeasure=0.19999999999999998)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.08088235294117647, recall=0.25, fmeasure=0.12222222222222223), mid=Score(precision=0.08088235294117647, recall=0.25, fmeasure=0.12222222222222223), high=Score(precision=0.08088235294117647, recall=0.25, fmeasure=0.12222222222222223)), 'rougeLsum': AggregateScore(low=Score(precision=0.08088235294117647, recall=0.25, fmeasure=0.12222222222222223), mid=Score(precision=0.08088235294117647, recall=0.25, fmeasure=0.12222222222222223), high=Score(precision=0.08088235294117647, recall=0.25, fmeasure=0.12222222222222223))}, {'rouge1': AggregateScore(low=Score(precision=0.06870229007633588, recall=0.391304347826087, fmeasure=0.1168831168831169), mid=Score(precision=0.06870229007633588, recall=0.391304347826087, fmeasure=0.1168831168831169), high=Score(precision=0.06870229007633588, recall=0.391304347826087, fmeasure=0.1168831168831169)), 'rouge2': AggregateScore(low=Score(precision=0.007692307692307693, recall=0.045454545454545456, fmeasure=0.013157894736842106), mid=Score(precision=0.007692307692307693, recall=0.045454545454545456, fmeasure=0.013157894736842106), high=Score(precision=0.007692307692307693, recall=0.045454545454545456, fmeasure=0.013157894736842106)), 'rougeL': AggregateScore(low=Score(precision=0.05343511450381679, recall=0.30434782608695654, fmeasure=0.09090909090909091), mid=Score(precision=0.05343511450381679, recall=0.30434782608695654, fmeasure=0.09090909090909091), high=Score(precision=0.05343511450381679, recall=0.30434782608695654, fmeasure=0.09090909090909091)), 'rougeLsum': AggregateScore(low=Score(precision=0.05343511450381679, recall=0.30434782608695654, fmeasure=0.09090909090909091), mid=Score(precision=0.05343511450381679, recall=0.30434782608695654, fmeasure=0.09090909090909091), high=Score(precision=0.05343511450381679, recall=0.30434782608695654, fmeasure=0.09090909090909091))}, {'rouge1': AggregateScore(low=Score(precision=0.056338028169014086, recall=0.1951219512195122, fmeasure=0.08743169398907105), mid=Score(precision=0.056338028169014086, recall=0.1951219512195122, fmeasure=0.08743169398907105), high=Score(precision=0.056338028169014086, recall=0.1951219512195122, fmeasure=0.08743169398907105)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.04929577464788732, recall=0.17073170731707318, fmeasure=0.07650273224043716), mid=Score(precision=0.04929577464788732, recall=0.17073170731707318, fmeasure=0.07650273224043716), high=Score(precision=0.04929577464788732, recall=0.17073170731707318, fmeasure=0.07650273224043716)), 'rougeLsum': AggregateScore(low=Score(precision=0.04929577464788732, recall=0.17073170731707318, fmeasure=0.07650273224043716), mid=Score(precision=0.04929577464788732, recall=0.17073170731707318, fmeasure=0.07650273224043716), high=Score(precision=0.04929577464788732, recall=0.17073170731707318, fmeasure=0.07650273224043716))}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def average_rouge(all_rouge):\n",
        "    avg_rouge = {'rouge-1': {'f': 0, 'p': 0, 'r': 0},\n",
        "                 'rouge-2': {'f': 0, 'p': 0, 'r': 0},\n",
        "                 'rouge-l': {'f': 0, 'p': 0, 'r': 0}}\n",
        "\n",
        "    n = len(all_rouge)\n",
        "\n",
        "    for rouge in all_rouge:\n",
        "        for metric in ['rouge-1', 'rouge-2', 'rouge-l']:\n",
        "            if metric in rouge:\n",
        "                for sub_metric in ['f', 'p', 'r']:\n",
        "                    if sub_metric in rouge[metric]:\n",
        "                        avg_rouge[metric][sub_metric] += rouge[metric][sub_metric]\n",
        "\n",
        "    for metric in ['rouge-1', 'rouge-2', 'rouge-l']:\n",
        "        for sub_metric in ['f', 'p', 'r']:\n",
        "            avg_rouge[metric][sub_metric] /= n\n",
        "\n",
        "    return avg_rouge\n",
        "\n"
      ],
      "metadata": {
        "id": "tiJujnixDrUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_rouge_scores = average_rouge(all_rouge)\n"
      ],
      "metadata": {
        "id": "9NvILo59DvBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def flatten_rouge_scores(all_rouge):\n",
        "    flattened_scores = []\n",
        "    for entry in all_rouge:\n",
        "        flat_entry = {}\n",
        "        for metric, aggregate_score in entry.items():\n",
        "            flat_entry[f\"{metric}_F1\"] = aggregate_score.mid.fmeasure\n",
        "            flat_entry[f\"{metric}_Precision\"] = aggregate_score.mid.precision\n",
        "            flat_entry[f\"{metric}_Recall\"] = aggregate_score.mid.recall\n",
        "        flattened_scores.append(flat_entry)\n",
        "    return flattened_scores\n",
        "\n",
        "flattened_scores = flatten_rouge_scores(all_rouge)\n",
        "df = pd.DataFrame(flattened_scores)\n",
        "#df\n",
        "average_values = df.mean()\n",
        "average_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wxmvn7sEIdu",
        "outputId": "e6873827-2376-4a37-cbc2-2949cc6f3628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rouge1_F1              0.247084\n",
              "rouge1_Precision       0.186782\n",
              "rouge1_Recall          0.467267\n",
              "rouge2_F1              0.128844\n",
              "rouge2_Precision       0.102316\n",
              "rouge2_Recall          0.214127\n",
              "rougeL_F1              0.187235\n",
              "rougeL_Precision       0.142005\n",
              "rougeL_Recall          0.363029\n",
              "rougeLsum_F1           0.187235\n",
              "rougeLsum_Precision    0.142005\n",
              "rougeLsum_Recall       0.363029\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}